{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf6d9ccd-6d17-429e-bca9-2bdaf4b3d4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ac.cdavies/.vnv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-09 15:06:58.422145: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744211218.436557 1375245 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744211218.440995 1375245 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744211218.453444 1375245 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744211218.453458 1375245 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744211218.453461 1375245 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744211218.453463 1375245 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-09 15:06:58.457231: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train', 'validation', 'test']\n",
      "['train', 'validation', 'test']\n",
      "In, Out, get_ipython, exit, quit, open, accelerate, datasets, evaluate, math, np, peft, pickle, pytest, ipytest, pd, transformers, load_dataset, load_dataset_builder, get_dataset_split_names, get_dataset_config_names, LoftQConfig, LoraConfig, get_peft_model, AutoModelForCausalLM, AutoTokenizer, pipeline, TrainingArguments, Trainer, SFTTrainer, SFTConfig, @py_builtins, @pytest_ar, model_from_pkl, ds_gst1_train, ds_gst1_test, ds_gst2_train, ds_gst2_test, search_data, search_with_strings, map_data, print_trainable_parameters, metric, comp_metrics_output, compute_metrics, make_trainer, get_dataframe, get_training_output, "
     ]
    }
   ],
   "source": [
    "%run training_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77259d6b-f8d1-46fe-b046-2a058f482db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74a214b0-5cdc-4af7-bd46-964c5c665adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lla_321, lla_321_tokenizer=model_from_pkl(\"Llama-3.2-1B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2c9f3b8-afb9-4745-a40f-95edb9b8714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_tokenized_stage1_train=map_data(ds_gst1_train, lla_321, lla_321_tokenizer)\n",
    "l_tokenized_stage1_test=map_data(ds_gst1_test, lla_321, lla_321_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62197cc1-2260-4315-b776-417e1d8e5607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader=DataLoader(l_tokenized_stage1_train, shuffle=True)\n",
    "test_dataloader=DataLoader(l_tokenized_stage1_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52d962a4-cb98-4948-886f-825cfe679126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['record', 'text', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 500\n",
      "})\n",
      "Dataset({\n",
      "    features: ['record', 'text', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(l_tokenized_stage1_train)\n",
    "print(l_tokenized_stage1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e290cc6e-f620-4cce-9834-8e0ae7cc5771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['record', 'text', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 500\n",
      "})\n",
      "Dataset({\n",
      "    features: ['record', 'text', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataloader.dataset)\n",
    "print(test_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ef6ce18-5a54-43ba-aac2-91a07be26300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to fine-tune with LoRA, instantiate a base model (as above)\n",
    "# create LoraConfig where LoRA-specific parameters are defined\n",
    "config=LoraConfig(\n",
    "    #inference_mode=False,\n",
    "    r=8, #rank of update matrices, lower value results in smaller matrices with fewer parameters\n",
    "    lora_alpha=16, #LoRA scaling factor\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.1, # dropout probability of LoRA layers\n",
    "    bias=\"none\", # specifies if bias parameters should be trained\n",
    "    #modules_to_save=[\"decode_head\"] #models apart from LoRA layers that are trainable\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfa50b52-1530-4a66-a025-c240017054b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lla_lora_model=get_peft_model(lla_321, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb82c7b4-cd1b-4853-adc4-cf4c2aaacf9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "              (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lla_lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85aaa5c4-e9b4-45be-be60-5c0bf0f98cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.AdamW(lla_lora_model.base_model.parameters(),\n",
    "                           amsgrad=False, # the AMSGrad variant of this algorithm won't be used \n",
    "                            betas=(0.9, 0.999), # coefficients used for computing running averages of gradient and its square\n",
    "                            capturable=False, # whether the instance will be captured in a CUDA graph\n",
    "                            differentiable=False, # whether autogad should occur through the optimzer step in training\n",
    "                            eps=1e-08, # added to denominator to improve numerical stablitity\n",
    "                            foreach=None, # whether foreach implementation is used\n",
    "                            fused=None, #whether the fused implementation is used\n",
    "                            #initial_lr=2e-05,\n",
    "                            lr=0.0, #learning rate\n",
    "                            maximize=False, # whether the object is maximized with respect to params instead og\n",
    "                            weight_decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b44c769-cd53-4e99-9fb2-a75491495a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opacus import PrivacyEngine\n",
    "privacy_engine = PrivacyEngine(secure_mode=False)\n",
    "model, optimizer, train_loader = privacy_engine.make_private(\n",
    "        module=lla_lora_model,\n",
    "        optimizer=optimizer,\n",
    "        data_loader=train_dataloader,\n",
    "        noise_multiplier=1.3,\n",
    "        max_grad_norm=1.0,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83dd55cf-80a6-4060-b85c-2a7717de9e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradSampleModule(PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "              (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       "))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "512843fa-5086-4bf2-8684-16a900ec0a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.optim.adamw.AdamW, {'lr': 2e-05, 'betas': (0.9, 0.999), 'eps': 1e-08})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l3_trainer.get_optimizer_cls_and_kwargs(SFTConfig(output_dir=\"test_trainer\", eval_strategy=\"epoch\",\n",
    "                                    per_device_train_batch_size=1,\n",
    "                                    max_grad_norm=1.0,\n",
    "                                    num_train_epochs=1,\n",
    "                                    logging_strategy=\"epoch\",\n",
    "                                    #logging_steps=6\n",
    "                                   ), (optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2aed7ae2-825e-44cf-b852-4652ba81e2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "opacus.optimizers.optimizer.DPOptimizer"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5b77f2e-9fcc-4592-964b-b55c1c23d96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dp_trainer=make_trainer(model, train_loader.dataset, test_dataloader.dataset, config,\n",
    "#                         SFTConfig(output_dir=\"test_trainer\", eval_strategy=\"epoch\",\n",
    "#                                     per_device_train_batch_size=1,\n",
    "#                                     num_train_epochs=1,\n",
    "#                                     logging_strategy=\"epoch\",\n",
    "#                                     #remove_unused_columns=False\n",
    "#                                     #logging_steps=6\n",
    "#                                    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da7df824-1eed-4f9b-8acb-a28db70c8336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dp_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4ae7392-a2a4-478b-9c50-2fedd12f75f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suggested solution\n",
    "model.prepare_inputs_for_generation = lla_lora_model.prepare_inputs_for_generation\n",
    "model.config=lla_lora_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76fb3ab7-477c-4a3e-b32c-679a06163472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 128000,\n",
       "  \"eos_token_id\": [\n",
       "    128001,\n",
       "    128008,\n",
       "    128009\n",
       "  ],\n",
       "  \"head_dim\": 64,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 8192,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"max_position_embeddings\": 131072,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 16,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": {\n",
       "    \"factor\": 32.0,\n",
       "    \"high_freq_factor\": 4.0,\n",
       "    \"low_freq_factor\": 1.0,\n",
       "    \"original_max_position_embeddings\": 8192,\n",
       "    \"rope_type\": \"llama3\"\n",
       "  },\n",
       "  \"rope_theta\": 500000.0,\n",
       "  \"tie_word_embeddings\": true,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.48.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 128256\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a58b46fc-9272-451b-a86b-cce81b7eb9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['record', 'text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2dbdb38-9c20-4d08-8af6-b1e0fa34acc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradSampleModule(PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "              (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       "))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8446dd6-4143-4463-a533-befc6494dfa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b4885a6-f813-41d6-bc8f-da73532bf032",
   "metadata": {},
   "outputs": [],
   "source": [
    "l3_trainer=make_trainer(lla_lora_model, train_dataloader.dataset, test_dataloader.dataset, config,\n",
    "                          SFTConfig(output_dir=\"test_trainer\", eval_strategy=\"epoch\",\n",
    "                                    per_device_train_batch_size=1,\n",
    "                                    max_grad_norm=1.0,\n",
    "                                    num_train_epochs=1,\n",
    "                                    logging_strategy=\"epoch\",\n",
    "                                    #logging_steps=6\n",
    "                                   ), (optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b78c9cc6-bb7d-45e9-b740-f917420da4ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "`AcceleratorState` object has no attribute `distributed_type`. This happens if `AcceleratorState._reset_state()` was called and an `Accelerator` or `PartialState` was not reinitialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ml3_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.vnv/lib/python3.12/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.vnv/lib/python3.12/site-packages/transformers/trainer.py:2200\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2198\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently training with a batch size of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2199\u001b[0m \u001b[38;5;66;03m# Data loader and number of training steps\u001b[39;00m\n\u001b[0;32m-> 2200\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_train_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_xla_v2_enabled:\n\u001b[1;32m   2202\u001b[0m     train_dataloader \u001b[38;5;241m=\u001b[39m tpu_spmd_dataloader(train_dataloader)\n",
      "File \u001b[0;32m~/.vnv/lib/python3.12/site-packages/transformers/trainer.py:1018\u001b[0m, in \u001b[0;36mTrainer.get_train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1015\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworker_init_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m seed_worker\n\u001b[1;32m   1016\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefetch_factor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_prefetch_factor\n\u001b[0;32m-> 1018\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataloader_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.vnv/lib/python3.12/site-packages/accelerate/accelerator.py:1294\u001b[0m, in \u001b[0;36mAccelerator.prepare\u001b[0;34m(self, device_placement, *args)\u001b[0m\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1284\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)\n\u001b[1;32m   1285\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_device_map(obj)\n\u001b[1;32m   1286\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mNO\n\u001b[1;32m   1287\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mACCELERATE_BYPASS_DEVICE_MAP\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1288\u001b[0m     ):\n\u001b[1;32m   1289\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1290\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt train a model that has been loaded with `device_map=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` in any distributed mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1291\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please rerun your script specifying `--num_processes=1` or by launching with `python \u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124mmyscript.py}}`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1292\u001b[0m         )\n\u001b[0;32m-> 1294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributed_type\u001b[49m \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   1295\u001b[0m     model_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1296\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m args:\n",
      "File \u001b[0;32m~/.vnv/lib/python3.12/site-packages/accelerate/accelerator.py:568\u001b[0m, in \u001b[0;36mAccelerator.distributed_type\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdistributed_type\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributed_type\u001b[49m\n",
      "File \u001b[0;32m~/.vnv/lib/python3.12/site-packages/accelerate/state.py:1125\u001b[0m, in \u001b[0;36mAcceleratorState.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# By this point we know that no attributes of `self` contain `name`,\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;66;03m# so we just modify the error message\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_attrs:\n\u001b[0;32m-> 1125\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1126\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`AcceleratorState` object has no attribute `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1127\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis happens if `AcceleratorState._reset_state()` was called and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1128\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man `Accelerator` or `PartialState` was not reinitialized.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1129\u001b[0m         )\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m# Raise a typical AttributeError\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAcceleratorState\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: `AcceleratorState` object has no attribute `distributed_type`. This happens if `AcceleratorState._reset_state()` was called and an `Accelerator` or `PartialState` was not reinitialized."
     ]
    }
   ],
   "source": [
    "l3_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3a311d-5193-48d6-8f66-77e2629a2b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_trainer = make_trainer(lla_lora_model, train_loader.dataset, test_dataloader.dataset, config,\n",
    "                          SFTConfig(output_dir=\"test_trainer\", eval_strategy=\"epoch\",\n",
    "                                    per_device_train_batch_size=1,\n",
    "                                    num_train_epochs=1,\n",
    "                                    logging_strategy=\"steps\",\n",
    "                                    #logging_steps=6\n",
    "                                   ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3985c95-6660-4325-a986-fab510ba299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a990de3b-a09b-4ad3-a393-fd23e03796f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function for using the optimizer\n",
    "l3_trainer.args.num_train_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eca567-fde8-4be2-8207-47c9ffedf827",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.grad_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb847732-9a85-41bd-9198-7b488946ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l3_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c38b008e-a4b3-4fa0-a4a0-be46117fccf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "opacus.optimizers.optimizer.DPOptimizer"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(l3_trainer.optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a72b0246-12af-47a3-a4b7-610b8b4171e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OptimizerPostHook',\n",
       " 'OptimizerPreHook',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_check_skip_next_step',\n",
       " '_cuda_graph_capture_health_check',\n",
       " '_get_flat_grad_sample',\n",
       " '_group_tensors_by_device_and_dtype',\n",
       " '_is_last_step_skipped',\n",
       " '_optimizer_step_code',\n",
       " '_patch_step_function',\n",
       " '_process_value_according_to_param_policy',\n",
       " '_step_skip_queue',\n",
       " 'accumulated_iterations',\n",
       " 'add_noise',\n",
       " 'add_param_group',\n",
       " 'attach_step_hook',\n",
       " 'clip_and_accumulate',\n",
       " 'defaults',\n",
       " 'expected_batch_size',\n",
       " 'generator',\n",
       " 'grad_samples',\n",
       " 'load_state_dict',\n",
       " 'loss_reduction',\n",
       " 'max_grad_norm',\n",
       " 'noise_multiplier',\n",
       " 'original_optimizer',\n",
       " 'param_groups',\n",
       " 'params',\n",
       " 'pre_step',\n",
       " 'profile_hook_step',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_load_state_dict_pre_hook',\n",
       " 'register_state_dict_post_hook',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'register_step_post_hook',\n",
       " 'register_step_pre_hook',\n",
       " 'scale_grad',\n",
       " 'secure_mode',\n",
       " 'signal_skip_step',\n",
       " 'state',\n",
       " 'state_dict',\n",
       " 'step',\n",
       " 'step_hook',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(l3_trainer.optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8be0df-a033-448d-90a1-247cc30a95e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(lla_lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c321c6aa-6351-4b4b-b678-7baaad0f6431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lla_model=AutoModelForCausalLM.from_pretrained('./fine_tuned/lla2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d6dae9-6af1-465b-9e4e-9ebab82e1708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(lla_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e116d3b7-f590-4078-be4e-5603cf866302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51de69a-e336-41c7-9886-4eff55b3d12d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
