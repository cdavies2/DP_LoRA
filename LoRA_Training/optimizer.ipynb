{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab6d699c-b39f-4f49-be14-3d723c450e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ac.cdavies/.vnv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-09 14:02:52.083735: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744207372.098133 1282883 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744207372.102390 1282883 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744207372.114469 1282883 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744207372.114485 1282883 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744207372.114488 1282883 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744207372.114491 1282883 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-09 14:02:52.118342: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train', 'validation', 'test']\n",
      "['train', 'validation', 'test']\n",
      "In, Out, get_ipython, exit, quit, open, accelerate, datasets, evaluate, math, np, peft, pickle, pytest, ipytest, pd, transformers, load_dataset, load_dataset_builder, get_dataset_split_names, get_dataset_config_names, LoftQConfig, LoraConfig, get_peft_model, AutoModelForCausalLM, AutoTokenizer, pipeline, TrainingArguments, Trainer, SFTTrainer, SFTConfig, @py_builtins, @pytest_ar, model_from_pkl, ds_gst1_train, ds_gst1_test, ds_gst2_train, ds_gst2_test, search_data, search_with_strings, map_data, print_trainable_parameters, metric, comp_metrics_output, compute_metrics, make_trainer, get_dataframe, get_training_output, "
     ]
    }
   ],
   "source": [
    "%run training_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6a752c8-c217-4b94-b714-904a97aedfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from opacus.grad_sample import GradSampleModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e76eebe-5962-4d1b-9b8e-79f448782038",
   "metadata": {},
   "outputs": [],
   "source": [
    "lla_321, lla_321_tokenizer=model_from_pkl(\"Llama-3.2-1B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09f3bb62-c001-461c-b7f3-d923ec5a44cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_tokenized_stage1_train=map_data(ds_gst1_train, lla_321, lla_321_tokenizer)\n",
    "l_tokenized_stage1_test=map_data(ds_gst1_test, lla_321, lla_321_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f0463da-a56a-4cff-a7d1-3bef3317a205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader=DataLoader(l_tokenized_stage1_train, shuffle=True)\n",
    "test_dataloader=DataLoader(l_tokenized_stage1_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "423b171f-016d-4a62-9ba0-4a1d297346df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to fine-tune with LoRA, instantiate a base model (as above)\n",
    "# create LoraConfig where LoRA-specific parameters are defined\n",
    "config=LoraConfig(\n",
    "    #inference_mode=False,\n",
    "    r=8, #rank of update matrices, lower value results in smaller matrices with fewer parameters\n",
    "    lora_alpha=16, #LoRA scaling factor\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.1, # dropout probability of LoRA layers\n",
    "    bias=\"none\", # specifies if bias parameters should be trained\n",
    "    #modules_to_save=[\"decode_head\"] #models apart from LoRA layers that are trainable\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ed40916-698d-4573-b4c1-e93981a7ca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "lla_lora_model=get_peft_model(lla_321, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7eb1107-587a-45c9-b6a7-924011e4767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.AdamW(lla_lora_model.base_model.parameters(),\n",
    "                           amsgrad=False, # the AMSGrad variant of this algorithm won't be used \n",
    "                            betas=(0.9, 0.999), # coefficients used for computing running averages of gradient and its square\n",
    "                            capturable=False, # whether the instance will be captured in a CUDA graph\n",
    "                            differentiable=False, # whether autogad should occur through the optimzer step in training\n",
    "                            eps=1e-08, # added to denominator to improve numerical stablitity\n",
    "                            foreach=None, # whether foreach implementation is used\n",
    "                            fused=None, #whether the fused implementation is used\n",
    "                            #initial_lr=2e-05,\n",
    "                            lr=0.0, #learning rate\n",
    "                            maximize=False, # whether the object is maximized with respect to params instead og\n",
    "                            weight_decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d43c7c86-294c-40ca-b13d-48cea8f93975",
   "metadata": {},
   "outputs": [],
   "source": [
    "l3_trainer=make_trainer(lla_lora_model, train_dataloader.dataset, test_dataloader.dataset, config,\n",
    "                          SFTConfig(output_dir=\"test_trainer\", eval_strategy=\"epoch\",\n",
    "                                    per_device_train_batch_size=1,\n",
    "                                    max_grad_norm=1.0,\n",
    "                                    num_train_epochs=1,\n",
    "                                    logging_strategy=\"epoch\",\n",
    "                                    #logging_steps=6\n",
    "                                   ), (optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1ae301c-10b5-43bf-bfd2-e12e1010e47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ac.cdavies/.vnv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 05:46, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.678200</td>\n",
       "      <td>2.678609</td>\n",
       "      <td>0.000527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ac.cdavies/.vnv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=0.678159423828125, metrics={'train_runtime': 350.5702, 'train_samples_per_second': 1.426, 'train_steps_per_second': 0.357, 'total_flos': 2992122101760000.0, 'train_loss': 0.678159423828125})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l3_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4a2603-aa75-4f8c-afc1-a0555a39ef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Trainer object will call the optimizer's train() function at each training step if method is callable/exists\n",
    "# maybe create a train method for your optimizer with the required steps for DPOptimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f5a6af5-6dbd-4a9f-9b5c-fc48d0108cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OptimizerPostHook',\n",
       " 'OptimizerPreHook',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_cuda_graph_capture_health_check',\n",
       " '_group_tensors_by_device_and_dtype',\n",
       " '_init_group',\n",
       " '_opt_called',\n",
       " '_optimizer_load_state_dict_post_hooks',\n",
       " '_optimizer_load_state_dict_pre_hooks',\n",
       " '_optimizer_state_dict_post_hooks',\n",
       " '_optimizer_state_dict_pre_hooks',\n",
       " '_optimizer_step_code',\n",
       " '_optimizer_step_post_hooks',\n",
       " '_optimizer_step_pre_hooks',\n",
       " '_patch_step_function',\n",
       " '_process_value_according_to_param_policy',\n",
       " '_warned_capturable_if_run_uncaptured',\n",
       " '_zero_grad_profile_name',\n",
       " 'add_param_group',\n",
       " 'defaults',\n",
       " 'load_state_dict',\n",
       " 'param_groups',\n",
       " 'profile_hook_step',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_load_state_dict_pre_hook',\n",
       " 'register_state_dict_post_hook',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'register_step_post_hook',\n",
       " 'register_step_pre_hook',\n",
       " 'state',\n",
       " 'state_dict',\n",
       " 'step',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(l3_trainer.optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39e5ee52-c4d8-425c-9fdd-cf656a0e75b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trl.trainer.sft_trainer.SFTTrainer"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(l3_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b1fed5f-b7e0-423f-845b-c2df3768402f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.optim.adamw.AdamW, {'lr': 2e-05, 'betas': (0.9, 0.999), 'eps': 1e-08})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the .get_optimizer_cls_and_kwargs on the trainer\n",
    "l3_trainer.get_optimizer_cls_and_kwargs(SFTConfig(output_dir=\"test_trainer\", eval_strategy=\"epoch\",\n",
    "                                    per_device_train_batch_size=1,\n",
    "                                    max_grad_norm=1.0,\n",
    "                                    num_train_epochs=1,\n",
    "                                    logging_strategy=\"epoch\",\n",
    "                                    #logging_steps=6\n",
    "                                   ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8da1137e-4b3b-4498-9520-2155ac6c3449",
   "metadata": {},
   "outputs": [],
   "source": [
    "l4_trainer=make_trainer(lla_lora_model, train_dataloader.dataset, test_dataloader.dataset, config,\n",
    "                          SFTConfig(output_dir=\"test_trainer\", eval_strategy=\"epoch\",\n",
    "                                    per_device_train_batch_size=1,\n",
    "                                    max_grad_norm=1.0,\n",
    "                                    num_train_epochs=1,\n",
    "                                    logging_strategy=\"epoch\",\n",
    "                                    #logging_steps=6\n",
    "                                   ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a247f5f1-ad97-4049-86db-1b9311ce4939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ac.cdavies/.vnv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 06:24, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.664300</td>\n",
       "      <td>2.611203</td>\n",
       "      <td>0.000859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ac.cdavies/.vnv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=0.664299072265625, metrics={'train_runtime': 385.8257, 'train_samples_per_second': 1.296, 'train_steps_per_second': 0.324, 'total_flos': 2992122101760000.0, 'train_loss': 0.664299072265625})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l4_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a3f686f-9552-4a88-8d71-27c68e35035c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OptimizerPostHook',\n",
       " 'OptimizerPreHook',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_cuda_graph_capture_health_check',\n",
       " '_group_tensors_by_device_and_dtype',\n",
       " '_init_group',\n",
       " '_opt_called',\n",
       " '_optimizer_load_state_dict_post_hooks',\n",
       " '_optimizer_load_state_dict_pre_hooks',\n",
       " '_optimizer_state_dict_post_hooks',\n",
       " '_optimizer_state_dict_pre_hooks',\n",
       " '_optimizer_step_code',\n",
       " '_optimizer_step_post_hooks',\n",
       " '_optimizer_step_pre_hooks',\n",
       " '_patch_step_function',\n",
       " '_process_value_according_to_param_policy',\n",
       " '_warned_capturable_if_run_uncaptured',\n",
       " '_zero_grad_profile_name',\n",
       " 'add_param_group',\n",
       " 'defaults',\n",
       " 'load_state_dict',\n",
       " 'param_groups',\n",
       " 'profile_hook_step',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_load_state_dict_pre_hook',\n",
       " 'register_state_dict_post_hook',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'register_step_post_hook',\n",
       " 'register_step_pre_hook',\n",
       " 'state',\n",
       " 'state_dict',\n",
       " 'step',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(l4_trainer.optimizer)\n",
    "# it seems that the grad_samples parameter doesn't exist in a regular AdamW optimizer, it is just for the DPOptimizer version\n",
    "# when the .get_optimizer_cls_and_kwargs function is ran on the trainer with the DPOptimizer, it retrieves arguments from the normal AdamW optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1b1aeb7-657c-44d2-9c71-72afb557b665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.optim.adamw.AdamW, {'lr': 2e-05, 'betas': (0.9, 0.999), 'eps': 1e-08})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l4_trainer.get_optimizer_cls_and_kwargs(SFTConfig(output_dir=\"test_trainer\", eval_strategy=\"epoch\",\n",
    "                                    per_device_train_batch_size=1,\n",
    "                                    max_grad_norm=1.0,\n",
    "                                    num_train_epochs=1,\n",
    "                                    logging_strategy=\"epoch\",\n",
    "                                    #logging_steps=6\n",
    "                                   ))\n",
    "# for the default and imported optimizers, the same arguments are retrieved by this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c2cdf1-7de3-4c1c-9eb0-82ecb3406ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
