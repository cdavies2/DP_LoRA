{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab6d699c-b39f-4f49-be14-3d723c450e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ac.cdavies/.vnv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-15 16:30:40.996093: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744734641.010120 1149232 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744734641.014389 1149232 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744734641.028014 1149232 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744734641.028030 1149232 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744734641.028034 1149232 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744734641.028037 1149232 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-15 16:30:41.031906: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'record': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None)}\n",
      "{'text': Value(dtype='string', id=None)}\n",
      "{'text': Value(dtype='string', id=None)}\n",
      "['train', 'validation', 'test']\n",
      "['train', 'validation', 'test']\n",
      "In, Out, get_ipython, exit, quit, open, accelerate, datasets, evaluate, math, np, os, peft, pickle, pytest, ipytest, pd, transformers, torch, time, threading, load_dataset, load_dataset_builder, get_dataset_split_names, get_dataset_config_names, LoftQConfig, LoraConfig, get_peft_model, AutoModelForCausalLM, AutoTokenizer, pipeline, TrainingArguments, Trainer, SFTTrainer, SFTConfig, @py_builtins, @pytest_ar, model_from_pkl, ds_builder1, ds_builder2, ds_builder3, ds_gs, ds_gst1_train, ds_gst1_test, ds_gst2_train, ds_gst2_test, search_with_strings, map_data, print_trainable_parameters, metric, initialize_heartbeat, heartbeat, end_heartbeat, comp_metrics_output, compute_metrics, make_trainer, get_dataframe, get_training_output, "
     ]
    }
   ],
   "source": [
    "%run training_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6a752c8-c217-4b94-b714-904a97aedfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from opacus.grad_sample import GradSampleModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e76eebe-5962-4d1b-9b8e-79f448782038",
   "metadata": {},
   "outputs": [],
   "source": [
    "lla_321, lla_321_tokenizer=model_from_pkl(\"Llama-3.2-1B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7452809-0144-410c-aab0-3fa3c4585340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train', 'validation', 'test']\n",
      "['train', 'validation', 'test']\n"
     ]
    }
   ],
   "source": [
    "ds_gst1_train=load_dataset(\"LongSafari/open-genome\", \"stage1\", split=\"train[:500]\")\n",
    "#print(ds_gst1[50])\n",
    "ds_gst1_test=load_dataset(\"LongSafari/open-genome\", \"stage1\", split=\"test[:50]\")\n",
    "print(get_dataset_split_names(\"LongSafari/open-genome\", \"stage1\"))\n",
    "ds_gst2_train=load_dataset(\"LongSafari/open-genome\", \"stage2\", split=\"train[:500]\")\n",
    "ds_gst2_test=load_dataset(\"LongSafari/open-genome\", \"stage2\", split=\"test[:50]\")\n",
    "print(get_dataset_split_names(\"LongSafari/open-genome\", \"stage2\"))\n",
    "# using a smaller dataset to make sure this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c20895bf-c995-4e5e-b42d-c8fd3aeacc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try to use a custom training loop\n",
    "def preprocess_data(dataset):\n",
    "    new_dataset=dataset.remove_columns([\"text\"]) # pytorch does not accept this input\n",
    "    new_dataset.set_format(\"torch\") # ensures Tensors are returned\n",
    "    return new_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1ec8d17-e2a7-4746-ae1d-9dd5aaac74af",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_tokenized_stage1_train=preprocess_data(l_tokenized_stage1_train)\n",
    "l_tokenized_stage1_test=preprocess_data(l_tokenized_stage1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b736b6c-8484-4bbe-98f8-771f5b13fceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader=DataLoader(l_tokenized_stage1_train, shuffle=True, batch_size=8)\n",
    "test_dataloader=DataLoader(l_tokenized_stage1_test, shuffle=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7eb1107-587a-45c9-b6a7-924011e4767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.AdamW(lla_lora_model.base_model.parameters(),\n",
    "                           amsgrad=False, # the AMSGrad variant of this algorithm won't be used \n",
    "                            betas=(0.9, 0.999), # coefficients used for computing running averages of gradient and its square\n",
    "                            capturable=False, # whether the instance will be captured in a CUDA graph\n",
    "                            differentiable=False, # whether autogad should occur through the optimzer step in training\n",
    "                            eps=1e-08, # added to denominator to improve numerical stablitity\n",
    "                            foreach=None, # whether foreach implementation is used\n",
    "                            fused=None, #whether the fused implementation is used\n",
    "                            #initial_lr=2e-05,\n",
    "                            lr=0.1, #learning rate\n",
    "                            maximize=False, # whether the object is maximized with respect to params instead og\n",
    "                            weight_decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e5ff46-d321-4f37-8f7f-639326cdc66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to fine-tune with LoRA, instantiate a base model (as above)\n",
    "# create LoraConfig where LoRA-specific parameters are defined\n",
    "config=LoraConfig(\n",
    "    #inference_mode=False,\n",
    "    r=8, #rank of update matrices, lower value results in smaller matrices with fewer parameters\n",
    "    lora_alpha=16, #LoRA scaling factor\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.1, # dropout probability of LoRA layers\n",
    "    bias=\"none\", # specifies if bias parameters should be trained\n",
    "    #modules_to_save=[\"decode_head\"] #models apart from LoRA layers that are trainable\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ed40916-698d-4573-b4c1-e93981a7ca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "lla_lora_model=get_peft_model(lla_321, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d5ce20-b319-46c2-ae8b-e9694eeb594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "l3_trainer=make_trainer(lla_lora_model, train_dataloader.dataset, test_dataloader.dataset, config,\n",
    "                          SFTConfig(output_dir=\"test_trainer\", eval_strategy=\"epoch\",\n",
    "                                    per_device_train_batch_size=1,\n",
    "                                    max_grad_norm=1.0,\n",
    "                                    num_train_epochs=1,\n",
    "                                    logging_strategy=\"epoch\",\n",
    "                                    #logging_steps=6\n",
    "                                   ), (optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3178092-cc53-48c6-ae17-bec55df00e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "def dp_train(trainer):\n",
    "    num_epochs=trainer.args.num_train_epochs\n",
    "    num_steps=num_epochs * len(train_dataloader)\n",
    "    scheduler=get_scheduler(\n",
    "                name=\"linear\",\n",
    "                optimizer=self.optimizer,\n",
    "                num_warmup_steps=0,\n",
    "                num_training_steps=num_training_steps,\n",
    "            ) #this is the default learning rate scheduler from the trainer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3890e719-844c-4cad-b2d1-f11f067811c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=\"cuda:0\" \n",
    "\n",
    "lla_lora_model=lla_lora_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1ae301c-10b5-43bf-bfd2-e12e1010e47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ac.cdavies/.vnv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.678200</td>\n",
       "      <td>2.678609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ac.cdavies/.vnv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=0.678159423828125, metrics={'train_runtime': 34.2864, 'train_samples_per_second': 14.583, 'train_steps_per_second': 3.646, 'total_flos': 2992122101760000.0, 'train_loss': 0.678159423828125})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l3_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9545cbc2-6850-4cef-8868-b64fa6d53b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "l3_trainer.state.log_history[0]['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae6790b-ba8d-49e8-937e-6bd6342f7e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "l3_trainer.train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6be7b3-3f3c-48d6-9cd3-cb28dc45c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(l3_trainer.prediction_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4a2603-aa75-4f8c-afc1-a0555a39ef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Trainer object will call the optimizer's train() function at each training step if method is callable/exists\n",
    "# maybe create a train method for your optimizer with the required steps for DPOptimizer \n",
    "def optimize(trainer):\n",
    "    #for i in range(l3_trainer.args.num_train_epochs): might not be needed if the trainer executes this at steps/epochs\n",
    "    losses=[] # empty loss list\n",
    "    data_loader=trainer.get_train_dataloader\n",
    "    optimizer=trainer.optimizer\n",
    "    optimizer.zero_grad() # resets the gradients of the parameter tensors to zero\n",
    "\n",
    "    outputs= trainer.#training output at this step/epoch\n",
    "    loss=outputs[0]['loss'] #appropriate position in outputs\n",
    "    loss.backward() # calculates gradient by backpropagating error from the current loss\n",
    "    losses.append(loss.item()) #add the gradient value to the list\n",
    "    optimizer.step() # perform optimization step to update the parameter\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5161df33-c4c3-4b11-af45-93812e060873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc48bd2-77ec-4f56-a34d-1135c5ada1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0fa4ec-c666-42d8-841a-9489af41a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.pre_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6658877f-6eb1-4579-9d60-35cd78d48231",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.train=optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec8e80-8bed-4113-acb1-1d1450e648b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opacus import PrivacyEngine\n",
    "privacy_engine = PrivacyEngine(secure_mode=False)\n",
    "model, optimizer, train_loader = privacy_engine.make_private(\n",
    "        module=lla_lora_model,\n",
    "        optimizer=optimizer,\n",
    "        data_loader=train_dataloader,\n",
    "        noise_multiplier=1.3,\n",
    "        max_grad_norm=1.0,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01780259-8eb2-4074-95a3-d9a4cfe289ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "lla_lora_model=get_peft_model(lla_321, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e385da9-2490-4a02-ba31-611b65540813",
   "metadata": {},
   "outputs": [],
   "source": [
    "l6_trainer=make_trainer(lla_lora_model, train_dataloader.dataset, test_dataloader.dataset, config,\n",
    "                          SFTConfig(output_dir=\"test_trainer\", eval_strategy=\"epoch\",\n",
    "                                    per_device_train_batch_size=1,\n",
    "                                    max_grad_norm=1.0,\n",
    "                                    num_train_epochs=1,\n",
    "                                    logging_strategy=\"epoch\",\n",
    "                                    #logging_steps=6,\n",
    "                                    ), optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8bc609-f06c-4b2a-a7ca-c585a742e55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "l6_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52123dac-13e0-4148-9554-2db8d29fc67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20707ea-ae1c-4c91-abdb-cd1d2e8a2be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "l6_trainer.optimizer.train(l5_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5daad3a-2c10-44cf-892b-da280c8e256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "l5_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a6af5-6dbd-4a9f-9b5c-fc48d0108cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(l3_trainer.optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e5ee52-c4d8-425c-9fdd-cf656a0e75b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(l3_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1fed5f-b7e0-423f-845b-c2df3768402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the .get_optimizer_cls_and_kwargs on the trainer\n",
    "l3_trainer.get_optimizer_cls_and_kwargs(SFTConfig(output_dir=\"test_trainer\", eval_strategy=\"epoch\",\n",
    "                                    per_device_train_batch_size=1,\n",
    "                                    max_grad_norm=1.0,\n",
    "                                    num_train_epochs=1,\n",
    "                                    logging_strategy=\"epoch\",\n",
    "                                    #logging_steps=6\n",
    "                                   ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da1137e-4b3b-4498-9520-2155ac6c3449",
   "metadata": {},
   "outputs": [],
   "source": [
    "l4_trainer=make_trainer(lla_lora_model, train_dataloader.dataset, test_dataloader.dataset, config,\n",
    "                          SFTConfig(output_dir=\"test_trainer\", eval_strategy=\"epoch\",\n",
    "                                    per_device_train_batch_size=1,\n",
    "                                    max_grad_norm=1.0,\n",
    "                                    num_train_epochs=1,\n",
    "                                    logging_strategy=\"epoch\",\n",
    "                                    #logging_steps=6\n",
    "                                   ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a247f5f1-ad97-4049-86db-1b9311ce4939",
   "metadata": {},
   "outputs": [],
   "source": [
    "l4_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3f686f-9552-4a88-8d71-27c68e35035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(l4_trainer.optimizer)\n",
    "# it seems that the grad_samples parameter doesn't exist in a regular AdamW optimizer, it is just for the DPOptimizer version\n",
    "# when the .get_optimizer_cls_and_kwargs function is ran on the trainer with the DPOptimizer, it retrieves arguments from the normal AdamW optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b1aeb7-657c-44d2-9c71-72afb557b665",
   "metadata": {},
   "outputs": [],
   "source": [
    "l4_trainer.get_optimizer_cls_and_kwargs(SFTConfig(output_dir=\"test_trainer\", eval_strategy=\"epoch\",\n",
    "                                    per_device_train_batch_size=1,\n",
    "                                    max_grad_norm=1.0,\n",
    "                                    num_train_epochs=1,\n",
    "                                    logging_strategy=\"epoch\",\n",
    "                                    #logging_steps=6\n",
    "                                   ))\n",
    "# for the default and imported optimizers, the same arguments are retrieved by this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c2cdf1-7de3-4c1c-9eb0-82ecb3406ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
