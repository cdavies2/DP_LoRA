{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab6d699c-b39f-4f49-be14-3d723c450e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train', 'validation', 'test']\n",
      "['train', 'validation', 'test']\n",
      "In, Out, get_ipython, exit, quit, open, accelerate, datasets, evaluate, math, np, peft, pickle, pytest, ipytest, pd, transformers, torch, load_dataset, load_dataset_builder, get_dataset_split_names, get_dataset_config_names, LoftQConfig, LoraConfig, get_peft_model, AutoModelForCausalLM, AutoTokenizer, pipeline, TrainingArguments, Trainer, SFTTrainer, SFTConfig, @py_builtins, @pytest_ar, model_from_pkl, ds_gst1_train, ds_gst1_test, ds_gst2_train, ds_gst2_test, search_with_strings, map_data, print_trainable_parameters, metric, comp_metrics_output, compute_metrics, make_trainer, get_dataframe, get_training_output, "
     ]
    }
   ],
   "source": [
    "%run training_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6a752c8-c217-4b94-b714-904a97aedfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from opacus.grad_sample import GradSampleModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e76eebe-5962-4d1b-9b8e-79f448782038",
   "metadata": {},
   "outputs": [],
   "source": [
    "lla_321, lla_321_tokenizer=model_from_pkl(\"Llama-3.2-1B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09f3bb62-c001-461c-b7f3-d923ec5a44cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_tokenized_stage1_train=map_data(ds_gst1_train, lla_321, lla_321_tokenizer)\n",
    "l_tokenized_stage1_test=map_data(ds_gst1_test, lla_321, lla_321_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f0463da-a56a-4cff-a7d1-3bef3317a205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader=DataLoader(l_tokenized_stage1_train, shuffle=True)\n",
    "test_dataloader=DataLoader(l_tokenized_stage1_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "423b171f-016d-4a62-9ba0-4a1d297346df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to fine-tune with LoRA, instantiate a base model (as above)\n",
    "# create LoraConfig where LoRA-specific parameters are defined\n",
    "config=LoraConfig(\n",
    "    #inference_mode=False,\n",
    "    r=8, #rank of update matrices, lower value results in smaller matrices with fewer parameters\n",
    "    lora_alpha=16, #LoRA scaling factor\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.1, # dropout probability of LoRA layers\n",
    "    bias=\"none\", # specifies if bias parameters should be trained\n",
    "    #modules_to_save=[\"decode_head\"] #models apart from LoRA layers that are trainable\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1ed40916-698d-4573-b4c1-e93981a7ca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "lla_lora_model=get_peft_model(lla_321, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e7eb1107-587a-45c9-b6a7-924011e4767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.AdamW(lla_lora_model.base_model.parameters(),\n",
    "                           amsgrad=False, # the AMSGrad variant of this algorithm won't be used \n",
    "                            betas=(0.9, 0.999), # coefficients used for computing running averages of gradient and its square\n",
    "                            capturable=False, # whether the instance will be captured in a CUDA graph\n",
    "                            differentiable=False, # whether autogad should occur through the optimzer step in training\n",
    "                            eps=1e-08, # added to denominator to improve numerical stablitity\n",
    "                            foreach=None, # whether foreach implementation is used\n",
    "                            fused=None, #whether the fused implementation is used\n",
    "                            #initial_lr=2e-05,\n",
    "                            lr=0.1, #learning rate\n",
    "                            maximize=False, # whether the object is maximized with respect to params instead og\n",
    "                            weight_decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d43c7c86-294c-40ca-b13d-48cea8f93975",
   "metadata": {},
   "outputs": [],
   "source": [
    "l3_trainer=make_trainer(lla_lora_model, train_dataloader.dataset, test_dataloader.dataset, config,\n",
    "                          SFTConfig(output_dir=\"test_trainer\", eval_strategy=\"epoch\",\n",
    "                                    per_device_train_batch_size=1,\n",
    "                                    max_grad_norm=1.0,\n",
    "                                    num_train_epochs=1,\n",
    "                                    logging_strategy=\"epoch\",\n",
    "                                    #logging_steps=6\n",
    "                                   ), (optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ae301c-10b5-43bf-bfd2-e12e1010e47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ac.cdavies/.vnv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='126' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ac.cdavies/.vnv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "l3_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9545cbc2-6850-4cef-8868-b64fa6d53b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6782"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l3_trainer.state.log_history[0]['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5ae2f273-2acd-49cc-a09a-7e3493b594fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "method"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fae6790b-ba8d-49e8-937e-6bd6342f7e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['record', 'text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l3_trainer.train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ae4a2603-aa75-4f8c-afc1-a0555a39ef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Trainer object will call the optimizer's train() function at each training step if method is callable/exists\n",
    "# maybe create a train method for your optimizer with the required steps for DPOptimizer \n",
    "def optimize(trainer):\n",
    "    #for i in range(l3_trainer.args.num_train_epochs): might not be needed if the trainer executes this at steps/epochs\n",
    "    losses=[] # empty loss list\n",
    "    data_loader=trainer.get_train_dataloader\n",
    "    optimizer=trainer.optimizer\n",
    "    optimizer.zero_grad() # resets the gradients of the parameter tensors to zero\n",
    "\n",
    "    outputs= trainer.state.log_history#training output at this step/epoch\n",
    "    loss=outputs[0]['loss'] #appropriate position in outputs\n",
    "    loss.backward() # calculates gradient by backpropagating error from the current loss\n",
    "    losses.append(loss.item()) #add the gradient value to the list\n",
    "    optimizer.step() # perform optimization step to update the parameter\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdc48bd2-77ec-4f56-a34d-1135c5ada1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method AdamW.step of AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.1\n",
       "    maximize: False\n",
       "    weight_decay: 0.0\n",
       ")>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb0fa4ec-c666-42d8-841a-9489af41a17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DPOptimizer.pre_step of AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    initial_lr: 0.1\n",
       "    lr: 0.1\n",
       "    maximize: False\n",
       "    weight_decay: 0.0\n",
       ")>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.pre_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6658877f-6eb1-4579-9d60-35cd78d48231",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.train=optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "83ec8e80-8bed-4113-acb1-1d1450e648b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opacus import PrivacyEngine\n",
    "privacy_engine = PrivacyEngine(secure_mode=False)\n",
    "model, optimizer, train_loader = privacy_engine.make_private(\n",
    "        module=lla_lora_model,\n",
    "        optimizer=optimizer,\n",
    "        data_loader=train_dataloader,\n",
    "        noise_multiplier=1.3,\n",
    "        max_grad_norm=1.0,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01780259-8eb2-4074-95a3-d9a4cfe289ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "lla_lora_model=get_peft_model(lla_321, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e385da9-2490-4a02-ba31-611b65540813",
   "metadata": {},
   "outputs": [],
   "source": [
    "l6_trainer=make_trainer(lla_lora_model, train_dataloader.dataset, test_dataloader.dataset, config,\n",
    "                          SFTConfig(output_dir=\"test_trainer\", eval_strategy=\"epoch\",\n",
    "                                    per_device_train_batch_size=1,\n",
    "                                    max_grad_norm=1.0,\n",
    "                                    num_train_epochs=1,\n",
    "                                    logging_strategy=\"epoch\",\n",
    "                                    #logging_steps=6,\n",
    "                                    ), optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52123dac-13e0-4148-9554-2db8d29fc67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OptimizerPostHook',\n",
       " 'OptimizerPreHook',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_check_skip_next_step',\n",
       " '_cuda_graph_capture_health_check',\n",
       " '_get_flat_grad_sample',\n",
       " '_group_tensors_by_device_and_dtype',\n",
       " '_is_last_step_skipped',\n",
       " '_opt_called',\n",
       " '_optimizer_step_code',\n",
       " '_patch_step_function',\n",
       " '_process_value_according_to_param_policy',\n",
       " '_step_skip_queue',\n",
       " 'accumulated_iterations',\n",
       " 'add_noise',\n",
       " 'add_param_group',\n",
       " 'attach_step_hook',\n",
       " 'clip_and_accumulate',\n",
       " 'defaults',\n",
       " 'expected_batch_size',\n",
       " 'generator',\n",
       " 'grad_samples',\n",
       " 'load_state_dict',\n",
       " 'loss_reduction',\n",
       " 'max_grad_norm',\n",
       " 'noise_multiplier',\n",
       " 'original_optimizer',\n",
       " 'param_groups',\n",
       " 'params',\n",
       " 'pre_step',\n",
       " 'profile_hook_step',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_load_state_dict_pre_hook',\n",
       " 'register_state_dict_post_hook',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'register_step_post_hook',\n",
       " 'register_step_pre_hook',\n",
       " 'scale_grad',\n",
       " 'secure_mode',\n",
       " 'signal_skip_step',\n",
       " 'state',\n",
       " 'state_dict',\n",
       " 'step',\n",
       " 'step_hook',\n",
       " 'train',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a20707ea-ae1c-4c91-abdb-cd1d2e8a2be4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ml6_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml5_trainer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 12\u001b[0m, in \u001b[0;36moptimize\u001b[0;34m(trainer)\u001b[0m\n\u001b[1;32m     10\u001b[0m outputs\u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mlog_history\u001b[38;5;66;03m#training output at this step/epoch\u001b[39;00m\n\u001b[1;32m     11\u001b[0m loss\u001b[38;5;241m=\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m#appropriate position in outputs\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m() \u001b[38;5;66;03m# calculates gradient by backpropagating error from the current loss\u001b[39;00m\n\u001b[1;32m     13\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem()) \u001b[38;5;66;03m#add the gradient value to the list\u001b[39;00m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "l6_trainer.optimizer.train(l5_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5daad3a-2c10-44cf-892b-da280c8e256c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ac.cdavies/.vnv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 06:12, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.664500</td>\n",
       "      <td>2.611787</td>\n",
       "      <td>0.000937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ac.cdavies/.vnv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=0.6644692993164063, metrics={'train_runtime': 373.4442, 'train_samples_per_second': 1.339, 'train_steps_per_second': 0.335, 'total_flos': 2992122101760000.0, 'train_loss': 0.6644692993164063})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l5_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f5a6af5-6dbd-4a9f-9b5c-fc48d0108cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OptimizerPostHook',\n",
       " 'OptimizerPreHook',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_cuda_graph_capture_health_check',\n",
       " '_group_tensors_by_device_and_dtype',\n",
       " '_init_group',\n",
       " '_opt_called',\n",
       " '_optimizer_load_state_dict_post_hooks',\n",
       " '_optimizer_load_state_dict_pre_hooks',\n",
       " '_optimizer_state_dict_post_hooks',\n",
       " '_optimizer_state_dict_pre_hooks',\n",
       " '_optimizer_step_code',\n",
       " '_optimizer_step_post_hooks',\n",
       " '_optimizer_step_pre_hooks',\n",
       " '_patch_step_function',\n",
       " '_process_value_according_to_param_policy',\n",
       " '_warned_capturable_if_run_uncaptured',\n",
       " '_zero_grad_profile_name',\n",
       " 'add_param_group',\n",
       " 'defaults',\n",
       " 'load_state_dict',\n",
       " 'param_groups',\n",
       " 'profile_hook_step',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_load_state_dict_pre_hook',\n",
       " 'register_state_dict_post_hook',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'register_step_post_hook',\n",
       " 'register_step_pre_hook',\n",
       " 'state',\n",
       " 'state_dict',\n",
       " 'step',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(l3_trainer.optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39e5ee52-c4d8-425c-9fdd-cf656a0e75b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trl.trainer.sft_trainer.SFTTrainer"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(l3_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b1fed5f-b7e0-423f-845b-c2df3768402f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.optim.adamw.AdamW, {'lr': 2e-05, 'betas': (0.9, 0.999), 'eps': 1e-08})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the .get_optimizer_cls_and_kwargs on the trainer\n",
    "l3_trainer.get_optimizer_cls_and_kwargs(SFTConfig(output_dir=\"test_trainer\", eval_strategy=\"epoch\",\n",
    "                                    per_device_train_batch_size=1,\n",
    "                                    max_grad_norm=1.0,\n",
    "                                    num_train_epochs=1,\n",
    "                                    logging_strategy=\"epoch\",\n",
    "                                    #logging_steps=6\n",
    "                                   ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8da1137e-4b3b-4498-9520-2155ac6c3449",
   "metadata": {},
   "outputs": [],
   "source": [
    "l4_trainer=make_trainer(lla_lora_model, train_dataloader.dataset, test_dataloader.dataset, config,\n",
    "                          SFTConfig(output_dir=\"test_trainer\", eval_strategy=\"epoch\",\n",
    "                                    per_device_train_batch_size=1,\n",
    "                                    max_grad_norm=1.0,\n",
    "                                    num_train_epochs=1,\n",
    "                                    logging_strategy=\"epoch\",\n",
    "                                    #logging_steps=6\n",
    "                                   ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a247f5f1-ad97-4049-86db-1b9311ce4939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ac.cdavies/.vnv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 06:24, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.664300</td>\n",
       "      <td>2.611203</td>\n",
       "      <td>0.000859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ac.cdavies/.vnv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=0.664299072265625, metrics={'train_runtime': 385.8257, 'train_samples_per_second': 1.296, 'train_steps_per_second': 0.324, 'total_flos': 2992122101760000.0, 'train_loss': 0.664299072265625})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l4_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a3f686f-9552-4a88-8d71-27c68e35035c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OptimizerPostHook',\n",
       " 'OptimizerPreHook',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_cuda_graph_capture_health_check',\n",
       " '_group_tensors_by_device_and_dtype',\n",
       " '_init_group',\n",
       " '_opt_called',\n",
       " '_optimizer_load_state_dict_post_hooks',\n",
       " '_optimizer_load_state_dict_pre_hooks',\n",
       " '_optimizer_state_dict_post_hooks',\n",
       " '_optimizer_state_dict_pre_hooks',\n",
       " '_optimizer_step_code',\n",
       " '_optimizer_step_post_hooks',\n",
       " '_optimizer_step_pre_hooks',\n",
       " '_patch_step_function',\n",
       " '_process_value_according_to_param_policy',\n",
       " '_warned_capturable_if_run_uncaptured',\n",
       " '_zero_grad_profile_name',\n",
       " 'add_param_group',\n",
       " 'defaults',\n",
       " 'load_state_dict',\n",
       " 'param_groups',\n",
       " 'profile_hook_step',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_load_state_dict_pre_hook',\n",
       " 'register_state_dict_post_hook',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'register_step_post_hook',\n",
       " 'register_step_pre_hook',\n",
       " 'state',\n",
       " 'state_dict',\n",
       " 'step',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(l4_trainer.optimizer)\n",
    "# it seems that the grad_samples parameter doesn't exist in a regular AdamW optimizer, it is just for the DPOptimizer version\n",
    "# when the .get_optimizer_cls_and_kwargs function is ran on the trainer with the DPOptimizer, it retrieves arguments from the normal AdamW optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1b1aeb7-657c-44d2-9c71-72afb557b665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.optim.adamw.AdamW, {'lr': 2e-05, 'betas': (0.9, 0.999), 'eps': 1e-08})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l4_trainer.get_optimizer_cls_and_kwargs(SFTConfig(output_dir=\"test_trainer\", eval_strategy=\"epoch\",\n",
    "                                    per_device_train_batch_size=1,\n",
    "                                    max_grad_norm=1.0,\n",
    "                                    num_train_epochs=1,\n",
    "                                    logging_strategy=\"epoch\",\n",
    "                                    #logging_steps=6\n",
    "                                   ))\n",
    "# for the default and imported optimizers, the same arguments are retrieved by this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c2cdf1-7de3-4c1c-9eb0-82ecb3406ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
