{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a30e97fe-15b9-47b5-afd9-530c9602c796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ac.cdavies/.vnv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' mistral/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/2dcff66eac0c01dc50e4c41eea959968232187fe/config.json'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "hf_hub_download(repo_id=\"mistralai/Mistral-7B-Instruct-v0.1\", filename=\"config.json\", cache_dir=\" mistral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "49039f82-9860-457a-a2ee-dd0cae2b176e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  3.98it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, pipeline\n",
    "#config=AutoConfig.from_pretrained(\" mistral/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/2dcff66eac0c01dc50e4c41eea959968232187fe/config.json\")\n",
    "model=AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", num_labels=3, torch_dtype=\"auto\")\n",
    "model.save_pretrained(\"mis_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc3cf242-dfd0-417e-ad9e-2440f932dcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████| 3/3 [00:07<00:00,  2.50s/it]\n"
     ]
    }
   ],
   "source": [
    "m2=AutoModelForCausalLM.from_pretrained(\"mis_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3de253f-7ba3-4bfc-a119-86baa0769e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mis_token/tokenizer_config.json',\n",
       " 'mis_token/special_tokens_map.json',\n",
       " 'mis_token/tokenizer.model',\n",
       " 'mis_token/added_tokens.json',\n",
       " 'mis_token/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "tokenizer.save_pretrained(\"mis_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "555b99c7-968f-4302-af51-548c05deb7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2=AutoTokenizer.from_pretrained(\"mis_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9a851c5c-86d5-4cd9-a4f0-117cf0cddeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time #check how many tokens per second are generated by a model\n",
    "def timeIt(model, prompt, tokenizer=None):\n",
    "    if (tokenizer==None):\n",
    "        pipe=pipeline(\"text-generation\", model=model)\n",
    "    else:\n",
    "        pipe=pipeline(\"text-generation\", model=model, tokenizer=tokenizer) #only specify tokenizer if one is separately imported\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt} \n",
    "    ]\n",
    "    start_time=time.time() # get the time before the text generation request is made to the model\n",
    "    output=pipe(messages, max_new_tokens=512)[0]['generated_text'][-1]['content'] # make request, retrieve the generated output\n",
    "    end_time=time.time() # get the time after completion\n",
    "    print(output)\n",
    "    total_time= end_time - start_time # subtract start and end time to get the time taken for generation\n",
    "    print(total_time, \"seconds\")\n",
    "    tokens=t2.tokenize(output) # convert output to tokens\n",
    "    token_len=len(tokens) # get number of tokens\n",
    "    tokens_per_sec=token_len/total_time # divide tokens by time to get tokens per second\n",
    "    print(tokens_per_sec, \" tokens per second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9984a447-d96b-4b84-9e99-7da99c897a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A tokenizer is a software tool that breaks down a string of text into smaller units, called tokens, which can be analyzed or processed individually. Tokens are typically words or phrases, but they can also be punctuation marks, numbers, or other types of characters. Tokenizers are commonly used in natural language processing (NLP) applications, such as text classification, sentiment analysis, and machine translation, to preprocess text data and make it more suitable for analysis. There are various types of tokenizers, including rule-based tokenizers, regular expression tokenizers, and machine learning-based tokenizers, each with its own strengths and weaknesses.\n",
      "3.174036979675293 seconds\n",
      "41.27236098345692  tokens per second\n"
     ]
    }
   ],
   "source": [
    "timeIt(m2, \"Please explain what a tokenizer is\", t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7e05daf2-4e32-47f6-9e8a-bb46af598996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A tokenizer is a crucial component in natural language processing (NLP) and computer science in general. In simpler terms, a tokenizer is a software component that breaks down text into individual tokens, which are the basic building blocks of language.\n",
      "\n",
      "Think of a tokenizer like a set of scissors that cuts a large piece of paper into smaller, manageable pieces. In the context of text processing, a tokenizer takes the raw text as input and breaks it down into tokens, such as words, punctuation, and special characters.\n",
      "\n",
      "Here's an example of how a tokenizer might work:\n",
      "\n",
      "* Input text: \"Hello, how are you?\"\n",
      "* Tokenizer: breaks down the text into tokens, such as:\n",
      "\t+ \"Hello\"\n",
      "\t+ \",\"\n",
      "\t+ \"how\"\n",
      "\t+ \"are\"\n",
      "\t+ \"you\"\n",
      "* Output: A list of tokens, which can be further processed by the NLP algorithm.\n",
      "\n",
      "Tokenizer types:\n",
      "\n",
      "1. **Word tokenizer**: breaks down text into individual words.\n",
      "2. **Character tokenizer**: breaks down text into individual characters.\n",
      "3. **Sentence tokenizer**: breaks down text into individual sentences.\n",
      "4. **Tokenization with special characters**: handles special characters, such as punctuation and spaces.\n",
      "\n",
      "Tokenizer algorithms:\n",
      "\n",
      "1. **Rule-based tokenization**: uses a set of predefined rules to split text into tokens.\n",
      "2. **Regular expression tokenization**: uses a regular expression to match patterns in text and split it into tokens.\n",
      "3. **Machine learning-based tokenization**: uses machine learning algorithms to learn patterns in text and predict tokenization.\n",
      "\n",
      "Tokenizer importance:\n",
      "\n",
      "1. **Text preprocessing**: tokenization is a critical step in text preprocessing, as it sets the stage for further analysis and processing.\n",
      "2. **Language understanding**: tokenization is essential for understanding the structure and meaning of language.\n",
      "3. **Information extraction**: tokenization is used in various applications, such as sentiment analysis, entity recognition, and topic modeling.\n",
      "\n",
      "In summary, a tokenizer is a vital component in text processing that breaks down text into individual tokens, which can be further analyzed and processed by NLP algorithms.\n",
      "5.83531379699707 seconds\n",
      "81.74367593486926  tokens per second\n"
     ]
    }
   ],
   "source": [
    "timeIt(\"meta-llama/Llama-3.2-1B-Instruct\", \"Please explain what a tokenizer is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94145f1-e0fb-48d6-ad63-4975250c4372",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
